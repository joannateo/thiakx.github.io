set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=cars$Price,p=0.7,list=F)
training = cars[inTrain,]
testing = cars[-inTrain,]
str(training)
modFitRF = train(training[,2:length(training)],training$Price,method="rf", importance = T) #randomForest, can take a while
summary(modFitRF)
summary(modFitRF$finalModel)
summary(modFitRF$finalModel)
modFitRF$finalModel
varImpt(modFitRF$finalModel)
varImp(modFitRF$finalModel)
modFitGBM = train(training[,2:length(training)],training$Price,method="gbm",verbose=F,
trControl = fitControl,tuneGrid = gbmGrid) #boosting tuned
modFitRR = train(training[,2:length(training)],training$Price,method="ridge") #ridge regression
modFitNet = train(training[,2:length(training)],training$Price,method="glmnet") #lasso/ridge/elastic net regression
#see what model was produced
modFitRF
modFitGBM
modFitRR
modFitNet
#see what variables are important, does not show +ve / -ve / no relationship with what we trying to predict
varImp(modFitRF)
varImp(modFitNet)
#now we can see relationships - for linear models
coefImp=coef(modFitNet$finalModel,modFitNet$bestTune$lambda)
coefImp=data.frame(row.names(coefImp),matrix(coefImp))
names(coefImp)=c("var","coef")
coefImp[order(abs(coefImp$coef),decreasing=T),]
#now we can see relationships - for non linear models (partial dependence plot)
plot(modFitGBM$finalModel, i.var = 1) #mileage is first variable, hecnce i.var=1
modFitRF = train(training[,2:length(training)],training$Price,method="rf", importance = T) #randomForest, can take a while
modFitRR = train(training[,2:length(training)],training$Price,method="ridge") #ridge regression
#optional: set multicore
library(cluster)
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1)
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
modFitRR = train(training[,2:length(training)],training$Price,method="ridge") #ridge regression
modFitRR = train(Price~.,data=training,method="ridge") #ridge regression
modFitNet = train(Price~.,data=training,method="glmnet") #lasso/ridge/elastic net regression
modFitNet
varImp(modFitNet)
modFitGBM = train(Price~.,data=training,method="gbm",verbose=F,
trControl = fitControl,tuneGrid = gbmGrid) #boosting tuned
#tune the model (advanced stuff, needs to be customized for different models)
#here we only tune gbm
gbmGrid =  expand.grid(interaction.depth = c(1, 5, 9), n.trees = (1:10)*50, shrinkage = 0.1)
fitControl = trainControl(method = "cv", number = 10)
modFitGBM = train(Price~.,data=training,method="gbm",verbose=F,
trControl = fitControl,tuneGrid = gbmGrid) #boosting tuned
modFitGBM
imp <- importance(modFitRF)
imp <- importance(modFitRF)
imp <- importance(modFitRF$bestTune)
modFitRF$bestTune
modFitRF
imp <- importance(modFitRF$finalModel)
impvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)]
impvar
op <- par(mfrow=c(2, 3))
for (i in seq_along(impvar)) {
partialPlot(modFitRF$finalModel, training, impvar[i], xlab=impvar[i],
main=paste("Partial Dependence on", impvar[i]),
ylim=c(30, 70))
}
par(op)
library(ggplot2)
library(caret)
data(cars)
#####regression example#####
#optional: set multicore
library(cluster)
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1)
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
#take a look at data...there seems to be a lot of 1s and 0s?
summary(cars)
# some of the columns are categorical (factors)
factorCols=c("Cruise","Sound","Leather","Buick","Cadillac","Chevy","Pontiac","Saab","Saturn",
"convertible","coupe","hatchback","sedan","wagon")
cars[factorCols] = data.frame(lapply(cars[factorCols], function(x) factor(x) ))
summary(cars)
# optional: normalize the numeric data (except for what we trying to predict, price)
# may improve model accuracy but makes it harder to interpret data.
# preObj = preProcess(cars[,c("Doors","Mileage","Cylinder")], method=c("scale","center"))
# cars[,c("Doors","Mileage","Cylinder")] = predict(preObj, cars[,c("Doors","Mileage","Cylinder")])
# summary(cars)
#prepare the data, 70% train, 30% test
set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=cars$Price,p=0.7,list=F)
training = cars[inTrain,]
testing = cars[-inTrain,]
modFitRR = train(Price~.,data=training,method="lda") #ridge regression
data(iris)
set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=iris$Species,p=0.7,list=F)
training = iris[inTrain,]
testing = iris[-inTrain,]
modFit = train(Species~.,data=training,method="lda")
coef(modFit)
coefImp=coef(modFitNet$finalModel)
coefImp=coef(modFit$finalModel)
coefImp=data.frame(row.names(coefImp),matrix(coefImp))
names(coefImp)=c("var","coef")
coefImp[order(abs(coefImp$coef),decreasing=T),]
modFit = train(Species~.,data=training,method="lda") #linear discriminant analysis
modFitLDA = train(Species~.,data=training,method="lda") #linear discriminant analysis
coefImp=coef(modFitLDA$finalModel)
coefImp=data.frame(row.names(coefImp),matrix(coefImp))
names(coefImp)=c("var","coef")
coefImp[order(abs(coefImp$coef),decreasing=T),]
View(iris)
coefImp=coef(modFitLDA$finalModel)
View(coefImp)
modFitLDA$bestTune$lambda
modFitLDA$bestTune
modFitLDA
modFitLDA$bestTune
modFitLDA$finalModel
coefImp=coef(modFitLDA$finalModel)[1]
coefImp=coef(modFitLDA$finalModel)[,1]
View(iris)
coefImp=coef(modFitLDA$finalModel)
coefImp=data.frame(row.names(coefImp),matrix(coefImp))
View(coefImp)
coefImp=coef(modFitLDA$finalModel)
coefImp=data.frame(coef(modFitLDA$finalModel))
View(coefImp)
View(coefImp)
data.frame(coef(modFitLDA$finalModel))
#ridge regression wont work on classification problem
library(ggplot2)
library(caret)
data(cars)
#####regression example#####
#optional: set multicore
library(cluster)
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1)
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
#take a look at data...there seems to be a lot of 1s and 0s?
summary(cars)
# some of the columns are categorical (factors)
factorCols=c("Cruise","Sound","Leather","Buick","Cadillac","Chevy","Pontiac","Saab","Saturn",
"convertible","coupe","hatchback","sedan","wagon")
cars[factorCols] = data.frame(lapply(cars[factorCols], function(x) factor(x) ))
summary(cars)
# optional: normalize the numeric data (except for what we trying to predict, price)
# may improve model accuracy but makes it harder to interpret data.
# preObj = preProcess(cars[,c("Doors","Mileage","Cylinder")], method=c("scale","center"))
# cars[,c("Doors","Mileage","Cylinder")] = predict(preObj, cars[,c("Doors","Mileage","Cylinder")])
# summary(cars)
#prepare the data, 70% train, 30% test
set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=cars$Price,p=0.7,list=F)
training = cars[inTrain,]
testing = cars[-inTrain,]
#tune the model (advanced stuff, needs to be customized for different models)
#here we only tune gbm
gbmGrid =  expand.grid(interaction.depth = c(1, 5, 9), n.trees = (1:10)*50, shrinkage = 0.1)
fitControl = trainControl(method = "cv", number = 10)
#fit the model
modFitRF = train(Price~.,data=training,method="rf", importance = T) #randomForest, can take a while
modFitGBM = train(Price~.,data=training,method="gbm",verbose=F,
trControl = fitControl,tuneGrid = gbmGrid) #boosting tuned
modFitRR = train(Price~.,data=training,method="ridge") #ridge regression
modFitNet = train(Price~.,data=training,method="glmnet") #lasso/ridge/elastic net regression
#see what model was produced
modFitRF
modFitGBM
modFitRR
modFitNet
#see what variables are important, does not show +ve / -ve / no relationship with what we trying to predict
varImp(modFitRF)
varImp(modFitNet)
#now we can see relationships - for linear models
coefImp=coef(modFitNet$finalModel,modFitNet$bestTune$lambda)
coefImp=data.frame(row.names(coefImp),matrix(coefImp))
names(coefImp)=c("var","coef")
coefImp[order(abs(coefImp$coef),decreasing=T),]
#now we can see relationships - for non linear models (partial dependence plot)
plot(modFitGBM$finalModel, i.var = 1) #only works for gbm, mileage is first variable, hecnce i.var=1
#now we can see relationships - for non linear models (partial dependence plot)
op = par(mfrow=c(2, 3))
plot(modFitGBM$finalModel, i.var = 1) #only works for gbm, mileage is first variable, hecnce i.var=1
plot(modFitGBM$finalModel, i.var = 2)
plot(modFitGBM$finalModel, i.var = 3)
plot(modFitGBM$finalModel, i.var = 4)
plot(modFitGBM$finalModel, i.var = 5)
plot(modFitGBM$finalModel, i.var = 6)
par(op)
op = par(mfrow=c(2, 3))
for (i in 1:6) {
plot(modFitGBM$finalModel, i.var = i) #only works for gbm, mileage is first variable, hecnce i.var=1
}
par(op)
modFitRF = train(training[,2:length(training)],training$price,method="rf", importance = T) #randomForest, can take a while
modFitRF = train(training[,2:length(training)],training$Price,method="rf", importance = T) #randomForest, can take a while
imp = importance(modFitRF)
imp = importance(modFitRF$finalModel)
impvar = rownames(imp)[order(imp[, 1], decreasing=TRUE)]
for (i in seq_along(impvar)) {
partialPlot(modFitRF, training, impvar[i], xlab=impvar[i],
main=paste("Partial Dependence on", impvar[i]),
ylim=c(30, 70))
}
par(op)
for (i in seq_along(impvar)) {
partialPlot(modFitRF$finalModel, training, impvar[i], xlab=impvar[i],
main=paste("Partial Dependence on", impvar[i]),
ylim=c(30, 70))
}
par(op)
#randomForest
op = par(mfrow=c(2, 3))
imp = importance(modFitRF$finalModel)
impvar = rownames(imp)[order(imp[, 1], decreasing=TRUE)]
for (i in seq_along(impvar)) {
partialPlot(modFitRF$finalModel, training, impvar[i], xlab=impvar[i],
main=paste("Partial Dependence on", impvar[i]),
ylim=c(30, 70))
}
par(op)
partialPlot(modFitRF$finalModel, training)
partialPlot(modFitRF$finalModel, training, impvar[i])
partialPlot(modFitRF$finalModel, training, impvar[1])
partialPlot(modFitRF$finalModel, training, impvar[2])
op = par(mfrow=c(2, 3))
imp = importance(modFitRF$finalModel)
impvar = rownames(imp)[order(imp[, 1], decreasing=TRUE)]
for (i in seq_along(impvar)) {
partialPlot(modFitRF$finalModel, training, impvar[i],
main=paste("Partial Dependence on", impvar[i]),
ylim=c(30, 70))
}
par(op)
rownames(training)
names(training)
partialPlot(modFitRF$finalModel, training, Mileage)
partialPlot(modFitRF$finalModel, training, Pontiac)
i.var
length(modFitGBM$finalModel)
op = par(mfrow=c(2, 3))
for (i in 1:(length(training)-1)) {
plot(modFitGBM$finalModel, i.var = i) #only works for gbm,i.var=variable number(i.var:1 = mileage)
}
par(op)
names(training)[2]
names(training)[3]
i=2
partialPlot(modFitRF$finalModel, training, names(training)[i])
#randomForest
op = par(mfrow=c(2, 3))
for (i in 2:length(training)) {
partialPlot(modFitRF$finalModel, training, names(training)[i])
plot(modFitGBM$finalModel, i.var = i) #only works for gbm,i.var=variable number(i.var:1 = mileage)
}
par(op)
#randomForest
op = par(mfrow=c(2, 3))
for (i in 2:length(training)) {
partialPlot(modFitRF$finalModel, training, names(training)[i],main=paste("Partial Dependence on", names(training)[i]),)
plot(modFitGBM$finalModel, i.var = i) #only works for gbm,i.var=variable number(i.var:1 = mileage)
}
par(op)
names(training)[1]
names(training)[18]
#randomForest
op = par(mfrow=c(2, 3))
for (i in 2:length(training)) {
partialPlot(modFitRF$finalModel, training, names(training)[i],main=paste("Partial Dependence on", names(training)[i]),)
}
par(op)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Predictive model using the 80% PCA predictors
modFitPCANet = train(diagnosis~. , data=training , method="glm", preProcess = "pca", trControl = trainControl(preProcOptions = list(thresh = 0.8)))
predictPCANet = predict(modFitPCANet, testing)
Matrix_PCANet <- confusionMatrix(predictPCANet, testing$diagnosis)
print(Matrix_PCANet)
list(thresh = 0.8)
modFitPCANet = train(diagnosis~. , data=training , method="glm", preProcess = "pca", trControl = trainControl(preProcOptions = list(thresh = 0.1)))
modFitPCANet = train(diagnosis~. , data=training , method="glm", preProcess = "pca", trControl = trainControl(preProcOptions = list(thresh = 0.99)))
warnings()
#####classifcation example#####
data(iris)
set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=iris$Species,p=0.7,list=F)
training = iris[inTrain,]
testing = iris[-inTrain,]
library(caret)
library(ggplot2)
#####classifcation example#####
data(iris)
set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=iris$Species,p=0.7,list=F)
training = iris[inTrain,]
testing = iris[-inTrain,]
trainingPCA=preProcess(training,method=c("pca","scale","center"),thresh=0.9)
View(training)
View(training)
trainingPCA=preProcess(training[,1:4],method=c("pca","scale","center"),thresh=0.9)
plot(trainingPCA)
plot(trainingPCA$rotation)
testingPCA=predict(trainingPCA,testing)
testingPCA=predict(trainingPCA,testing[,1:4])
View(testingPCA)
trainingPCA=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(trainingPCA,testing[,1:4])
trainingPCA$Species=training$Species
View(training)
View(testingPCA)
trainingPCA=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(trainingPCA,testing[,1:4])
trainingPCA$Species=training$Species
trainingPCA=trainingPCA$rotation
View(trainingPCA)
View(training)
View(trainingPCA)
PCA=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
trainingPCA=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
trainingPCA=trainingPCA$x
View(testingPCA)
#Step 1: Reduce number of dimensions to 2 (set it via pcaComp) and apply it to test data
trainingPCA=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(trainingPCA,testing[,1:4])
#Step 2: obtain the PCA data frame
trainingPCA=trainingPCA$x
testingPCA=trainingPCA$x
trainingPCA$Species=training$Species
testingPCA=testingPCA$x
#Step 1: Reduce number of dimensions to 2 (set it via pcaComp) and apply it to test data
trainingPCA=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(trainingPCA,testing[,1:4])
#Step 2: obtain the PCA data frame
trainingPCA=trainingPCA$x
testingPCA=testingPCA$x
#Step 1: Reduce number of dimensions to 2 (set it via pcaComp) and apply it to test data
trainingPCA=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(trainingPCA,testing[,1:4])
trainingPCA=predict(trainingPCA,trainingPCA[,1:4])
trainingPCA=predict(trainingPCA,trainingPCA)
trainingPCA=predict(trainingPCA,training[,1:4])
View(trainingPCA)
trainingPCA=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
x=trainingPCA$x
PCAMod=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(PCAMod,testing[,1:4])
trainingPCA=predict(PCAMod,training[,1:4])
trainingPCA$Species=training$Species
View(trainingPCA)
modFitLDA = train(Species~.,data=trainingPCA,method="lda") #linear discriminant analysis
pred=predict(modFitLDA,testingPCA)
View(testing)
confusionMatrix(pred, testing$Species)
modFitLDA = train(Species~.,data=training,method="rrlda") #linear discriminant analysis
modFitLDA = train(Species~.,data=training,method="rrlda") #linear discriminant analysis
pred=predict(modFitLDA,testing)
confusionMatrix(pred, testing$Species)
modFitLDA = train(Species~.,data=training,method="lda") #linear discriminant analysis
pred=predict(modFitLDA,testing)
confusionMatrix(pred, testing$Species)
data(iris)
set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=iris$Species,p=0.7,list=F)
training = iris[inTrain,]
testing = iris[-inTrain,]
#Step 1: Reduce number of dimensions to 2 (set it via pcaComp) and apply it to test data
PCAMod=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(PCAMod,testing[,1:4])
trainingPCA=predict(PCAMod,training[,1:4])
trainingPCA$Species=training$Species #remember to add back species (dependent variable)
modFitLDA = train(Species~.,data=trainingPCA,method="logreg") #linear discriminant analysis
modFitLDA = train(Species~.,data=trainingPCA,method="glm",family="binomial") #linear discriminant analysis
warnings()
modFitLDA = train(Species~.,data=trainingPCA,method="glm") #linear discriminant analysis
warnings()
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Predictive model using the 80% PCA predictors
modFitPCANet = train(diagnosis~. , data=training , method="glm", preProcess = "pca", trControl = trainControl(preProcOptions = list(thresh = 0.8)))
predictPCANet = predict(modFitPCANet, testing)
Matrix_PCANet <- confusionMatrix(predictPCANet, testing$diagnosis)
print(Matrix_PCANet)
#Step 1: Reduce number of dimensions to 2 (set it via pcaComp) and apply it to test data
PCAMod=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(PCAMod,testing[,1:4])
trainingPCA=predict(PCAMod,training[,1:4])
trainingPCA$Species=training$Species #remember to add back species (dependent variable)
#Step 2: train using PCA adjusted data
modFitRF= train(Species~.,data=trainingPCA,method="rf")
pred=predict(modFitRF,testingPCA)
confusionMatrix(pred, testing$Species)
#####classifcation example#####
data(iris)
set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=iris$Species,p=0.7,list=F)
training = iris[inTrain,]
testing = iris[-inTrain,]
#Step 1: Reduce number of dimensions to 2 (set it via pcaComp) and apply it to test data
PCAMod=preProcess(training[,1:4],method=c("pca","scale","center"),pcaComp=2)
testingPCA=predict(PCAMod,testing[,1:4])
trainingPCA=predict(PCAMod,training[,1:4])
trainingPCA$Species=training$Species #remember to add back species (dependent variable)
#Step 2: train using PCA adjusted data
modFitRF= train(Species~.,data=trainingPCA,method="rf")
pred=predict(modFitRF,testingPCA)
confusionMatrix(pred, testing$Species)
library(ggplot2)
library(caret)
data(cars)
#####regression example#####
#optional: set multicore
library(cluster)
library(parallel)
library(doSNOW)
coreNumber=max(detectCores(),1)
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
#take a look at data...there seems to be a lot of 1s and 0s?
summary(cars)
# some of the columns are categorical (factors)
factorCols=c("Cruise","Sound","Leather","Buick","Cadillac","Chevy","Pontiac","Saab","Saturn",
"convertible","coupe","hatchback","sedan","wagon")
cars[factorCols] = data.frame(lapply(cars[factorCols], function(x) factor(x) ))
summary(cars)
# optional: normalize the numeric data (except for what we trying to predict, price)
# may improve model accuracy but makes it harder to interpret data.
# preObj = preProcess(cars[,c("Doors","Mileage","Cylinder")], method=c("scale","center"))
# cars[,c("Doors","Mileage","Cylinder")] = predict(preObj, cars[,c("Doors","Mileage","Cylinder")])
# summary(cars)
#prepare the data, 70% train, 30% test
set.seed(1234) #there are randomness in createPartiaion, rf and gbm, this helps make our answers consistent
inTrain = createDataPartition(y=cars$Price,p=0.7,list=F)
training = cars[inTrain,]
testing = cars[-inTrain,]
#tune the model (advanced stuff, needs to be customized for different models)
#here we only tune gbm
gbmGrid =  expand.grid(interaction.depth = c(1, 5, 9), n.trees = (1:10)*50, shrinkage = 0.1)
fitControl = trainControl(method = "cv", number = 10)
ptm = proc.time()
modFitRR = train(Price~.,data=training,method="ridge") #ridge regression
proc.time() - ptm
proc.time()
install.packages("manipulate")
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
install.packages("rCharts")
library(rCharts)
library(devtools)
install_github("rCharts", "ramnathv")
library(rCharts)
load(airquality)
import(airquality)
library(airquality)
load(airquality)
data=airquality
dTable(airquality, sPaginationType = "full_numbers")
library(shiny)
runExample("01_hello")
runExample("01_hello")
runApp("App-1")
setwd("~/Documents/coursera developing data product/shiny")
setwd("~/Documents/coursera/developing data product/shiny")
runApp("App-1")
runApp("App-1")
runApp("App-2")
runApp("App-2")
install.packages(c("maps", "mapproj"))
runApp("census-app")
runApp("stockVis")
runApp("stockVis")
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='thiakx', token='D9C9AB65AD8721357F40681920FF143D', secret='Uj1xGh5xjKlSGo0C1zvFRQls79jDVB2Mmx57kuvn')
library(shinyapps)
shinyapps::deployApp('stockVis')
shinyapps::deployApp('simple')
shinyapps::deployApp('census')
#from shiny tutorial: http://shiny.rstudio.com/tutorial/
#make sure the libs are isntalled: "shiny","maps", "mapproj", "shinyapps"
setwd("~/Documents/coursera/developing data product/shiny")
library(shiny)
shinyapps::deployApp('stockVis')
runExample("01_hello") # a histogram
runGitHub( "stockVisDemo", "thiakx")
runGitHub( "stockVisDemo", "thiakx")
